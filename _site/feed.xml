<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://duanyc.top//feed.xml" rel="self" type="application/atom+xml" /><link href="https://duanyc.top//" rel="alternate" type="text/html" /><updated>2021-01-25T21:36:46+08:00</updated><id>https://duanyc.top//feed.xml</id><title type="html">DuanYuchen’s Blog.</title><subtitle>Your Site Description
</subtitle><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><entry><title type="html">Adversarial Examples on Graph Data(Deep Insights into Attack and Defense)</title><link href="https://duanyc.top//2021/01/25/Adversarial-Examples-on-Graph-Data(Deep-Insights-into-Attack-and-Defense).html" rel="alternate" type="text/html" title="Adversarial Examples on Graph Data(Deep Insights into Attack and Defense)" /><published>2021-01-25T00:00:00+08:00</published><updated>2021-01-25T00:00:00+08:00</updated><id>https://duanyc.top//2021/01/25/Adversarial%20Examples%20on%20Graph%20Data(Deep%20Insights%20into%20Attack%20and%20Defense)</id><content type="html" xml:base="https://duanyc.top//2021/01/25/Adversarial-Examples-on-Graph-Data(Deep-Insights-into-Attack-and-Defense).html">&lt;h1 id=&quot;adversarial-examples-on-graph-data-deep-insights-into-attack-and-defense&quot;&gt;Adversarial Examples on Graph Data: Deep Insights into Attack and Defense&lt;/h1&gt;

&lt;p&gt;Huijun Wu1,2, Chen Wang2, Yuriy Tyshetskiy2, Andrew Docherty2, Kai Lu3, Liming Zhu1,2
1University of New South Wales, Australia
2Data61, CSIRO
3National University of Defense Technology, China
{first, second}@data61.csiro.au, kailu@nudt.edu.cn&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;攻击：引入积分梯度，积分梯度被用作度量来测量在图G中扰动特定特征或边的优先级。对于具有高扰动优先级的特征或边，只需将其翻转为不同的二进制值即可对其进行扰动。&lt;/li&gt;
  &lt;li&gt;防御：在训练之前对给定的图进行预处理：检查图的邻接矩阵并检查边。选择连接具有低相似性分数的节点的所有边作为要移除的候选。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;research-objectives&quot;&gt;Research Objective(s)&lt;/h2&gt;

&lt;p&gt;​		在本文中，我们提出了攻击和防御两种技术。对于攻击，我们证明了通过引入积分梯度可以很容易地解决离散性问题，它可以准确地反映扰动某些特征或边缘的效果，同时仍然受益于并行计算。在防御方面，我们观察到目标攻击的恶意操纵图在统计上与正常图不同。基于这一观察结果，我们提出了一种检测图形并恢复潜在敌方扰动的防御方法。&lt;/p&gt;

&lt;p&gt;​		&lt;strong&gt;现有的攻击方法中：添加 连接着具有不同特征的节点的边 在所有攻击方法中都起着关键作用。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;​		&lt;strong&gt;本文证明了对图的邻接矩阵进行简单的预处理就能识别出被操纵的边。对于具有bag of word(BOW)特征的节点，Jaccard索引在度量连接节点之间的相似性时是有效的。通过去除连接非常不同节点的边，我们能够在不降低GCN模型的准确性的情况下防御有针对性的对抗性攻击。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h2&gt;

&lt;p&gt;​        图的深度学习模型，如图卷积网络(GCN)，在处理图数据的任务上取得了显著的性能。与其他类型的深度模型类似，图深度学习模型经常遭受敌意攻击。然而，与非图数据相比，图数据的离散特征、图的连通性以及对潜伏扰动的不同定义给图数据的对抗性攻防带来了独特的挑战和机遇。&lt;/p&gt;

&lt;p&gt;​		将在非图数据上的攻击方法，用在GCN上的挑战是离散输入问题。具体地说，图形节点的特征通常是离散的。边，特别是未加权图中的边，也是离散的。为了解决这个问题，基于贪婪的方法（下面的两篇文章&lt;a href=&quot;[Wang et al., 2018] Xiaoyun Wang, Joe Eaton, Cho-Jui Hsieh, and Felix Wu.&quot;&gt;1&lt;/a&gt;和&lt;a href=&quot;[Zügner et al., 2018] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann.&quot;&gt;2&lt;/a&gt;）来攻击基于图的深度学习系统。一种反复扰乱特征或图形结构的贪婪方法。图结构和特征统计在贪婪攻击期间被保留。在本文中，我们证明了尽管存在离散输入问题，但积分梯度仍然可以精确地逼近梯度。&lt;/p&gt;

&lt;h2 id=&quot;methods&quot;&gt;Method(s)&lt;/h2&gt;

&lt;p&gt;​		&lt;strong&gt;攻击：现有的攻击方法中，添加 连接着具有不同特征的节点的边 在所有攻击方法中都起着关键作用。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;​		&lt;strong&gt;本文证明了对图的邻接矩阵进行简单的预处理就能识别出被操纵的边。对于具有bag of word(BOW)特征的节点，Jaccard索引在度量连接节点之间的相似性时是有效的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;​		&lt;strong&gt;防御：通过去除连接非常不同节点的边，我们能够在不降低GCN模型的准确性的情况下防御有针对性的对抗性攻击。&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;integrated-gradients-guided-attack&quot;&gt;Integrated Gradients Guided Attack：&lt;/h3&gt;

&lt;p&gt;图表中的节点特征通常是词袋类型的特征，可以是1，也可以是0。图中未加权的边也经常用来表示特定关系的存在，因此邻接矩阵中只有1或0。当攻击该模型时，敌意扰动被限制为将1改为0，或者反之亦然。在图模型中应用普通的FGSM和JSMA的主要问题是梯度不准确。&lt;/p&gt;

&lt;p&gt;给定目标节点t，对于FGSM攻击，$\nabla J_{W^{(1)},W^{(2)}}(t)=\frac{\sigma J_{W^{(1)},W^{(2)}}(t)}{\sigma X}$ ∂X测量所有节点对损失函数值的特征重要性。这里，X是特征矩阵，它的每一行描述了图中一个节点的特征。对于节点n的特定特征i，$\nabla J_{W^{(1)},W^{(2)}}$的较大值表示扰动特征i为1，有助于使目标节点分类错误。但是，遵循此梯度可能没有用，原因有两个：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先，特征值可能已经是1，因此我们不能再对其进行扰动；&lt;/li&gt;
  &lt;li&gt;其次，即使特征值是0，由于GCN模型可能无法学习该特征值在0和1之间的局部线性函数，所以这种扰动的结果是不可预测的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;换句话说，原始的梯度存在局部梯度问题。（以一个简单的RELU网络f(X)=relu(X)为例，当x从0增加到1时，函数值也增加了1，但在x=0时计算梯度为0，不能准确地捕捉到模型的行为。）&lt;/p&gt;

&lt;p&gt;​		积分梯度定义如下：考虑一条从x0到输入x的直线路径，积分梯度是通过累加路径上所有点的所有梯度来获得的。&lt;/p&gt;

&lt;p&gt;​		形式上，x的第i个特征，其积分梯度Integrate Gradient（IG）如下：
\(IG_i(F(X))::=(x_i-\acute{x_i}) \times \int_{\alpha=0}^{1} \frac{\partial F(x'+\alpha x(x-x'))}{\partial x_i}d\alpha\)
在给定邻接矩阵A、特征矩阵X和目标节点t的情况下，我们计算函数$F_{W^{(1)},W^{(2)}}(A,X,t)$，其中I是攻击的输入。I=A表示边缘攻击，而I=X表示特征攻击。当F为GCN模型的损失函数时，我们将这种攻击技术称为积分梯度类FGSM攻击，即IG-FGSM。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于有目标的的IG-JSMA或IG-FGSM攻击，优化目标是最大化F的值。因此，对于值为1的特征或边，我们选择IG得分最低的特征/边，并将其扰动为0。&lt;/li&gt;
  &lt;li&gt;非目标IG-JSMA攻击旨在最小化获胜类的预测得分，以便我们尝试将IG得分高的输入维度增加到0。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;为了删除边：我们要对全0矩阵逐渐增加边，来达到目前的状态，所以要把A或者X设置为全0矩阵；
为了添加边：我们要对全1矩阵逐渐删除边，来达到目前的状态，所以要把A或者X设置为全1矩阵。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/duanyuchen55/ImageHosting/jekyll_pic/20210125213201.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;算法1显示了非目标IG-JSMA攻击的伪代码。我们计算获胜类别c的预测分数的积分梯度，即A和X的条目。
然后，积分梯度被用作度量来测量在图G中扰动特定特征或边的优先级。注意，边和特征值被考虑，并且仅计算可能扰动的分数(见等式(7))。（例如，我们只计算在以前不存在边的情况下添加边的重要性。）因此，对于具有高扰动优先级的特征或边，我们只需将其翻转为不同的二进制值即可对其进行扰动。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/duanyuchen55/ImageHosting/jekyll_pic/20210125213201.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;defense-for-adversarial-attack&quot;&gt;Defense for Adversarial Attack：&lt;/h3&gt;

&lt;p&gt;由于现有的针对GCN的attack都是有效的，因为attacked graph被直接用于训练新模型。基于此，一种可行的defense方法是使得邻接矩阵变得可训练。&lt;/p&gt;

&lt;p&gt;按照nettack的方法，初始化adversarial graph，然后直接训练GCN模型。通过如此简单的防御方法，攻击后目标节点被正确分类的可信度高达0.912。&lt;/p&gt;

&lt;p&gt;上述防御有效的原因：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;对边进行扰动比修改特征更有效。攻击方法倾向于添加边而不是删除边；&lt;/li&gt;
  &lt;li&gt;邻居较多的节点比邻居较少的节点更难攻击。这也与[Zügner等人，2018年]中的观察结果一致，即度越高的节点在干净图和被攻击图中的分类精度都更高。&lt;/li&gt;
  &lt;li&gt;攻击倾向于将目标节点连接到具有不同特征和标签的节点。我们发现这是最有效的攻击方式。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;使用 CORA-ML数据集来验证，由于CORA-ML数据集的特征是bag of word，所以我们使用Jaccard相似度来衡量特征之间的相似性。
\(J_{u,v}=\frac{M_{11}}{M_{01}+M_{10}+M_{11}}\)
$M_{01}$是feature number，其中特征值在节点u中为0，而在节点v中为1。其他类似。&lt;/p&gt;

&lt;p&gt;下图中可以看到，Adversarial Attack显著增加了与目标节点相似度得分较低的邻居节点的数量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/duanyuchen55/ImageHosting/jekyll_pic/20210125213247.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图神经网络本质上是根据图形结构聚合特征。对于目标节点，恶意创建的图试图将具有不同特征和标签的节点连接起来，以污染目标节点的表示，从而使目标节点与其正确类中的节点不那么相似。&lt;/p&gt;

&lt;p&gt;相应地，在删除边的同时，攻击倾向于删除连接与目标节点有许多相似之处的节点的边。边缘攻击更有效，因为添加或移除一条边会影响聚合过程中的所有特征维度。相反，修改一个特征只影响特征向量中的一个维度，并且这种扰动很容易被高度节点的其他邻居掩盖。&lt;/p&gt;

&lt;p&gt;基于这些观察结果，我们提出了另一个&lt;strong&gt;假设，即上述防御方法之所以有效，是因为模型为连接目标节点的边赋予了较低的权重，这些边连接到与目标节点特征相似度较低的节点。&lt;/strong&gt;为了验证这一点，我们绘制了从目标节点开始的边的末端节点的学习权重和Jaccard相似性得分(参见下图)。请注意，对于我们选择的目标节点，目标节点的每个邻居与其自身之间的Jaccard相似性得分在干净的图中大于0。相似度得分为零的边都是由攻击添加的。正如预期的那样，该模型对大部分相似度分数较低的边学习到了低权重。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/duanyuchen55/ImageHosting/jekyll_pic/20210125213409.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;为了使defense更有效，我们甚至不需要使用可学习的边权重作为defense。边权值的学习不可避免地会给模型引入额外的参数，这可能会影响模型的可扩展性和准确性。&lt;/strong&gt;基于以下几点，一种简单的方法可能同样有效：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;普通节点通常不会连接到许多与其没有相似之处的节点；&lt;/li&gt;
  &lt;li&gt;学习过程基本上是将较低的权重分配给连接两个不同节点的边。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==&lt;strong&gt;我们在训练之前对给定的图进行预处理：检查图的邻接矩阵并检查边。选择连接具有低相似性分数(例如，=0)的节点的所有边作为要移除的候选。&lt;/strong&gt;==（虽然clean graph也可能有少量这样的边，但我们发现删除这些边对目标节点的预测几乎没有什么坏处。相反，在某些情况下，删除这些边缘可能会改善预测。这是很直观的，因为聚合来自与目标截然不同的节点的特征通常会过度平滑节点表示。）&lt;/p&gt;

&lt;p&gt;简化反而可能会导致性能提升，例如这些工作：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/duanyuchen55/ImageHosting/jekyll_pic/20210125213619.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;p&gt;作者如何评估自己的方法，实验的setup是什么样的，有没有问题或者可以借鉴的地方。&lt;/p&gt;

&lt;p&gt;我们使用了广泛使用的CORA-ML、CITESEER和Polblog数据集。&lt;/p&gt;

&lt;p&gt;我们将每个图分为已标记节点(20%)和未标记节点(80%)。在标记的节点中，一半用于训练，另一半用于验证。对于Polblog数据集，由于没有特征属性，我们将属性矩阵设置为单位矩阵。&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;图形神经网络(GNN)显著提高了对多种类型图形数据的分析性能。然而，与其他类型数据中的深度神经网络一样，GNN也存在健壮性问题。本文对图卷积网络(GCN)中的鲁棒性问题进行了深入研究。我们提出了一种综合的基于梯度的攻击方法，在攻击性能上优于现有的迭代和基于梯度的攻击方法。我们还分析了针对GCN的攻击，发现健壮性问题根源于GCN中的本地聚集。为了提高GCN模型的稳健性，我们给出了一种有效的防御方法。我们在基准数据上验证了我们方法的有效性和高效性。&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Attack graph convolutional networks by adding fake nodes. 
arXiv preprint arXiv:1810.10751,2018&lt;/p&gt;

&lt;p&gt;Adversarial attacks on neural networks for graph data.
In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining, pages 2847–2856. ACM, 2018.&lt;/p&gt;</content><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><category term="Paper" /><category term="GCN" /><category term="Graph" /><category term="Adversarial" /><summary type="html">Adversarial Examples on Graph Data: Deep Insights into Attack and Defense Huijun Wu1,2, Chen Wang2, Yuriy Tyshetskiy2, Andrew Docherty2, Kai Lu3, Liming Zhu1,2 1University of New South Wales, Australia 2Data61, CSIRO 3National University of Defense Technology, China {first, second}@data61.csiro.au, kailu@nudt.edu.cn Summary 攻击：引入积分梯度，积分梯度被用作度量来测量在图G中扰动特定特征或边的优先级。对于具有高扰动优先级的特征或边，只需将其翻转为不同的二进制值即可对其进行扰动。 防御：在训练之前对给定的图进行预处理：检查图的邻接矩阵并检查边。选择连接具有低相似性分数的节点的所有边作为要移除的候选。 Research Objective(s) ​ 在本文中，我们提出了攻击和防御两种技术。对于攻击，我们证明了通过引入积分梯度可以很容易地解决离散性问题，它可以准确地反映扰动某些特征或边缘的效果，同时仍然受益于并行计算。在防御方面，我们观察到目标攻击的恶意操纵图在统计上与正常图不同。基于这一观察结果，我们提出了一种检测图形并恢复潜在敌方扰动的防御方法。 ​ 现有的攻击方法中：添加 连接着具有不同特征的节点的边 在所有攻击方法中都起着关键作用。 ​ 本文证明了对图的邻接矩阵进行简单的预处理就能识别出被操纵的边。对于具有bag of word(BOW)特征的节点，Jaccard索引在度量连接节点之间的相似性时是有效的。通过去除连接非常不同节点的边，我们能够在不降低GCN模型的准确性的情况下防御有针对性的对抗性攻击。 Problem Statement ​ 图的深度学习模型，如图卷积网络(GCN)，在处理图数据的任务上取得了显著的性能。与其他类型的深度模型类似，图深度学习模型经常遭受敌意攻击。然而，与非图数据相比，图数据的离散特征、图的连通性以及对潜伏扰动的不同定义给图数据的对抗性攻防带来了独特的挑战和机遇。 ​ 将在非图数据上的攻击方法，用在GCN上的挑战是离散输入问题。具体地说，图形节点的特征通常是离散的。边，特别是未加权图中的边，也是离散的。为了解决这个问题，基于贪婪的方法（下面的两篇文章1和2）来攻击基于图的深度学习系统。一种反复扰乱特征或图形结构的贪婪方法。图结构和特征统计在贪婪攻击期间被保留。在本文中，我们证明了尽管存在离散输入问题，但积分梯度仍然可以精确地逼近梯度。 Method(s) ​ 攻击：现有的攻击方法中，添加 连接着具有不同特征的节点的边 在所有攻击方法中都起着关键作用。 ​ 本文证明了对图的邻接矩阵进行简单的预处理就能识别出被操纵的边。对于具有bag of word(BOW)特征的节点，Jaccard索引在度量连接节点之间的相似性时是有效的。 ​ 防御：通过去除连接非常不同节点的边，我们能够在不降低GCN模型的准确性的情况下防御有针对性的对抗性攻击。 Integrated Gradients Guided Attack： 图表中的节点特征通常是词袋类型的特征，可以是1，也可以是0。图中未加权的边也经常用来表示特定关系的存在，因此邻接矩阵中只有1或0。当攻击该模型时，敌意扰动被限制为将1改为0，或者反之亦然。在图模型中应用普通的FGSM和JSMA的主要问题是梯度不准确。 给定目标节点t，对于FGSM攻击，$\nabla J_{W^{(1)},W^{(2)}}(t)=\frac{\sigma J_{W^{(1)},W^{(2)}}(t)}{\sigma X}$ ∂X测量所有节点对损失函数值的特征重要性。这里，X是特征矩阵，它的每一行描述了图中一个节点的特征。对于节点n的特定特征i，$\nabla J_{W^{(1)},W^{(2)}}$的较大值表示扰动特征i为1，有助于使目标节点分类错误。但是，遵循此梯度可能没有用，原因有两个： 首先，特征值可能已经是1，因此我们不能再对其进行扰动； 其次，即使特征值是0，由于GCN模型可能无法学习该特征值在0和1之间的局部线性函数，所以这种扰动的结果是不可预测的。 换句话说，原始的梯度存在局部梯度问题。（以一个简单的RELU网络f(X)=relu(X)为例，当x从0增加到1时，函数值也增加了1，但在x=0时计算梯度为0，不能准确地捕捉到模型的行为。） ​ 积分梯度定义如下：考虑一条从x0到输入x的直线路径，积分梯度是通过累加路径上所有点的所有梯度来获得的。 ​ 形式上，x的第i个特征，其积分梯度Integrate Gradient（IG）如下： \(IG_i(F(X))::=(x_i-\acute{x_i}) \times \int_{\alpha=0}^{1} \frac{\partial F(x'+\alpha x(x-x'))}{\partial x_i}d\alpha\) 在给定邻接矩阵A、特征矩阵X和目标节点t的情况下，我们计算函数$F_{W^{(1)},W^{(2)}}(A,X,t)$，其中I是攻击的输入。I=A表示边缘攻击，而I=X表示特征攻击。当F为GCN模型的损失函数时，我们将这种攻击技术称为积分梯度类FGSM攻击，即IG-FGSM。 对于有目标的的IG-JSMA或IG-FGSM攻击，优化目标是最大化F的值。因此，对于值为1的特征或边，我们选择IG得分最低的特征/边，并将其扰动为0。 非目标IG-JSMA攻击旨在最小化获胜类的预测得分，以便我们尝试将IG得分高的输入维度增加到0。 为了删除边：我们要对全0矩阵逐渐增加边，来达到目前的状态，所以要把A或者X设置为全0矩阵； 为了添加边：我们要对全1矩阵逐渐删除边，来达到目前的状态，所以要把A或者X设置为全1矩阵。 算法1显示了非目标IG-JSMA攻击的伪代码。我们计算获胜类别c的预测分数的积分梯度，即A和X的条目。 然后，积分梯度被用作度量来测量在图G中扰动特定特征或边的优先级。注意，边和特征值被考虑，并且仅计算可能扰动的分数(见等式(7))。（例如，我们只计算在以前不存在边的情况下添加边的重要性。）因此，对于具有高扰动优先级的特征或边，我们只需将其翻转为不同的二进制值即可对其进行扰动。 Defense for Adversarial Attack： 由于现有的针对GCN的attack都是有效的，因为attacked graph被直接用于训练新模型。基于此，一种可行的defense方法是使得邻接矩阵变得可训练。 按照nettack的方法，初始化adversarial graph，然后直接训练GCN模型。通过如此简单的防御方法，攻击后目标节点被正确分类的可信度高达0.912。 上述防御有效的原因： 对边进行扰动比修改特征更有效。攻击方法倾向于添加边而不是删除边； 邻居较多的节点比邻居较少的节点更难攻击。这也与[Zügner等人，2018年]中的观察结果一致，即度越高的节点在干净图和被攻击图中的分类精度都更高。 攻击倾向于将目标节点连接到具有不同特征和标签的节点。我们发现这是最有效的攻击方式。 使用 CORA-ML数据集来验证，由于CORA-ML数据集的特征是bag of word，所以我们使用Jaccard相似度来衡量特征之间的相似性。 \(J_{u,v}=\frac{M_{11}}{M_{01}+M_{10}+M_{11}}\) $M_{01}$是feature number，其中特征值在节点u中为0，而在节点v中为1。其他类似。 下图中可以看到，Adversarial Attack显著增加了与目标节点相似度得分较低的邻居节点的数量。 图神经网络本质上是根据图形结构聚合特征。对于目标节点，恶意创建的图试图将具有不同特征和标签的节点连接起来，以污染目标节点的表示，从而使目标节点与其正确类中的节点不那么相似。 相应地，在删除边的同时，攻击倾向于删除连接与目标节点有许多相似之处的节点的边。边缘攻击更有效，因为添加或移除一条边会影响聚合过程中的所有特征维度。相反，修改一个特征只影响特征向量中的一个维度，并且这种扰动很容易被高度节点的其他邻居掩盖。 基于这些观察结果，我们提出了另一个假设，即上述防御方法之所以有效，是因为模型为连接目标节点的边赋予了较低的权重，这些边连接到与目标节点特征相似度较低的节点。为了验证这一点，我们绘制了从目标节点开始的边的末端节点的学习权重和Jaccard相似性得分(参见下图)。请注意，对于我们选择的目标节点，目标节点的每个邻居与其自身之间的Jaccard相似性得分在干净的图中大于0。相似度得分为零的边都是由攻击添加的。正如预期的那样，该模型对大部分相似度分数较低的边学习到了低权重。 为了使defense更有效，我们甚至不需要使用可学习的边权重作为defense。边权值的学习不可避免地会给模型引入额外的参数，这可能会影响模型的可扩展性和准确性。基于以下几点，一种简单的方法可能同样有效： 普通节点通常不会连接到许多与其没有相似之处的节点； 学习过程基本上是将较低的权重分配给连接两个不同节点的边。 ==我们在训练之前对给定的图进行预处理：检查图的邻接矩阵并检查边。选择连接具有低相似性分数(例如，=0)的节点的所有边作为要移除的候选。==（虽然clean graph也可能有少量这样的边，但我们发现删除这些边对目标节点的预测几乎没有什么坏处。相反，在某些情况下，删除这些边缘可能会改善预测。这是很直观的，因为聚合来自与目标截然不同的节点的特征通常会过度平滑节点表示。） 简化反而可能会导致性能提升，例如这些工作： Evaluation 作者如何评估自己的方法，实验的setup是什么样的，有没有问题或者可以借鉴的地方。 我们使用了广泛使用的CORA-ML、CITESEER和Polblog数据集。 我们将每个图分为已标记节点(20%)和未标记节点(80%)。在标记的节点中，一半用于训练，另一半用于验证。对于Polblog数据集，由于没有特征属性，我们将属性矩阵设置为单位矩阵。 Conclusion 图形神经网络(GNN)显著提高了对多种类型图形数据的分析性能。然而，与其他类型数据中的深度神经网络一样，GNN也存在健壮性问题。本文对图卷积网络(GCN)中的鲁棒性问题进行了深入研究。我们提出了一种综合的基于梯度的攻击方法，在攻击性能上优于现有的迭代和基于梯度的攻击方法。我们还分析了针对GCN的攻击，发现健壮性问题根源于GCN中的本地聚集。为了提高GCN模型的稳健性，我们给出了一种有效的防御方法。我们在基准数据上验证了我们方法的有效性和高效性。 Notes References Attack graph convolutional networks by adding fake nodes. arXiv preprint arXiv:1810.10751,2018 Adversarial attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining, pages 2847–2856. ACM, 2018.</summary></entry><entry><title type="html">《深度学习推荐系统》笔记</title><link href="https://duanyc.top//2021/01/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0.html" rel="alternate" type="text/html" title="《深度学习推荐系统》笔记" /><published>2021-01-24T00:00:00+08:00</published><updated>2021-01-24T00:00:00+08:00</updated><id>https://duanyc.top//2021/01/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0</id><content type="html" xml:base="https://duanyc.top//2021/01/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0.html">&lt;h2 id=&quot;37-fm与深度学习模型的结合&quot;&gt;3.7 FM与深度学习模型的结合&lt;/h2&gt;

&lt;h3 id=&quot;371-fnn用fm的隐向量完成embedding层初始化&quot;&gt;3.7.1 FNN——用FM的隐向量完成Embedding层初始化&lt;/h3&gt;

&lt;p&gt;​		神经网络的参数初始化通常采用不包含任何先验信息的随机初始化，而Embedding层的输入极端稀疏化，导致Embedding层收敛缓慢，且Embedding层参数量占绝大部分，进而导致模型收敛受限于Embedding层。&lt;/p&gt;

&lt;p&gt;​		FNN模型解决上述问题的思路：使用FM模型训练好的各特征隐向量初始化Embedding层的参数（引入先验信息）&lt;strong&gt;书p79：图3-8及下面第一段话&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;​		FNN模型也为Embedding预训练提供了借鉴思路。&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;h3 id=&quot;372-deepfm用fm代替wide部分&quot;&gt;3.7.2 DeepFM——用FM代替Wide部分&lt;/h3&gt;

&lt;p&gt;​		FNN把FM的训练结果作为初始化权重，并未对神经网络的结构进行更改。&lt;/p&gt;

&lt;p&gt;​		DeepFM对Wide&amp;amp;Deep的改进在于：用FM替换了Wide部分，加强了浅层网络部分特征组合的能力。&lt;strong&gt;书p80：图3-9及下面第一段话&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;​		针对Wide&amp;amp;Deep的改进动机，DeepFM和Deep&amp;amp;Cross完全一样，只不过进行特征组合的方法不一样，前者使用FM，后者使用多层Cross网络。&lt;/p&gt;

&lt;h3 id=&quot;373-nfmfm的神经网络化尝试&quot;&gt;3.7.3 NFM——FM的神经网络化尝试&lt;/h3&gt;

&lt;p&gt;​		在数学形式上，NFM模型的主要思路是用一个表达能力更强的函数代替FM中的二阶隐向量内积的部分&lt;/p&gt;

\[\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N{w_ix_i}+\sum_{i=1}^N\sum_{j=i+1}^N{v_i^tv_j\cdot x_ix_j}\]

\[\hat{y}_{NFM}(x)=w_0+\sum_{i=1}^{N}w_ix_i+f(x)\]

&lt;p&gt;​		$f(x)$的构造工作可以交由某个深度学习网络来完成，并且通过BP来学习。&lt;/p&gt;

&lt;p&gt;​		NFM的这个神经网络架构特点：在Embedding层和多层神经网络之间加入特征交叉池化层。&lt;/p&gt;

&lt;p&gt;​		若把NFM的一阶部分看作一个线性模型，则NFM架构也可以视为Wide&amp;amp;Deep模型的进化。NFM模型对Wide&amp;amp;Deep模型的Deep部分加入了特征交叉池化层，加强了特征交叉。&lt;/p&gt;

&lt;h3 id=&quot;374-基于fm的深度学习模型的优点和局限性&quot;&gt;3.7.4 基于FM的深度学习模型的优点和局限性&lt;/h3&gt;

&lt;p&gt;​		FNN、DeepFM、NFM都是在经典的多层神经网络的基础上加入有针对性的特征交叉操作，使模型具有更强的非线性表达能力。&lt;/p&gt;

&lt;p&gt;​		特征工程的思路已经穷尽了可能的尝试，提升空间很小。&lt;/p&gt;

&lt;h2 id=&quot;38-注意力机制在推荐模型中的应用&quot;&gt;3.8 注意力机制在推荐模型中的应用&lt;/h2&gt;

&lt;h3 id=&quot;381-afm引入注意力机制的fm&quot;&gt;3.8.1 AFM——引入注意力机制的FM&lt;/h3&gt;

&lt;p&gt;​		注意力机制，基于假设——“不同的交叉特征”对于结果的影响程度不同。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现的。注意力网络的作用是为每一个交叉特征提供权重（注意力得分）&lt;/p&gt;

&lt;p&gt;​		为了防止交叉特征数据稀疏带来权重参数难以收敛的问题，AFM使用了一个粥两两特征交叉层和池化层之间的注意力网络来生成注意力得分。&lt;/p&gt;

&lt;h3 id=&quot;382-din引入注意力机制的深度学习网络&quot;&gt;3.8.2 DIN——引入注意力机制的深度学习网络&lt;/h3&gt;

&lt;p&gt;​		应用场景：阿里巴巴电商广告推荐。&lt;/p&gt;

&lt;p&gt;​		计算一个用户u是否点击广告a时，模型的输入特征分为两类：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;用户u的特征组。用户特征组里的商品id/商铺id序列，表示用户点击过的商品/商铺集合。&lt;/li&gt;
  &lt;li&gt;候选广告a的特征组。广告特征里的商品id和商铺id代表：广告对应的商品id和商铺id。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;利用候选商品和历史性为商品之间的相关性计算出一个权重，该权重就代表了“注意力”的强弱。注意力权重+深度学习网络=DIN模型，注意力形式化表达如下：
\(V_u = f(V_a)=\sum_{i=1}^{N}{w_i \cdot V_i}=\sum_{i=1}^{N}{g(V_i, V_a)} \cdot V_i\)
$V_i$是用户的Embedding向量。$V_a$是候选广告商品的Embedding向量。$V_i$是用户u的第i次行为的Embedding向量。&lt;/p&gt;

&lt;p&gt;用户的行为就是浏览商品和店铺，因此行为的Embedding向量就是那次浏览商品或者店铺的Embedding向量。&lt;/p&gt;

&lt;p&gt;$g(V_i,V_a)$使用一个激活单元activation unit来生成注意力得分，输入层是两个Embedding向量，经过元素减操作后，与原Embedding向量一同连接后形成全链接层的输入，最后通过单神经元输出层生成注意力得分。&lt;/p&gt;

&lt;h3 id=&quot;383-注意力机制对推荐系统的启发&quot;&gt;3.8.3 注意力机制对推荐系统的启发&lt;/h3&gt;

&lt;p&gt;​		注意力机制在数学形式上，只是将过去的平均操作或加和操作 换成了 加权求和或加权平均操作。&lt;/p&gt;

&lt;h2 id=&quot;39-dien序列模型与推荐系统的结合&quot;&gt;3.9 DIEN——序列模型与推荐系统的结合&lt;/h2&gt;</content><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><category term="DL" /><category term="RS" /><summary type="html">3.7 FM与深度学习模型的结合 3.7.1 FNN——用FM的隐向量完成Embedding层初始化 ​ 神经网络的参数初始化通常采用不包含任何先验信息的随机初始化，而Embedding层的输入极端稀疏化，导致Embedding层收敛缓慢，且Embedding层参数量占绝大部分，进而导致模型收敛受限于Embedding层。 ​ FNN模型解决上述问题的思路：使用FM模型训练好的各特征隐向量初始化Embedding层的参数（引入先验信息）书p79：图3-8及下面第一段话 ​ FNN模型也为Embedding预训练提供了借鉴思路。 ​ 3.7.2 DeepFM——用FM代替Wide部分 ​ FNN把FM的训练结果作为初始化权重，并未对神经网络的结构进行更改。 ​ DeepFM对Wide&amp;amp;Deep的改进在于：用FM替换了Wide部分，加强了浅层网络部分特征组合的能力。书p80：图3-9及下面第一段话 ​ 针对Wide&amp;amp;Deep的改进动机，DeepFM和Deep&amp;amp;Cross完全一样，只不过进行特征组合的方法不一样，前者使用FM，后者使用多层Cross网络。 3.7.3 NFM——FM的神经网络化尝试 ​ 在数学形式上，NFM模型的主要思路是用一个表达能力更强的函数代替FM中的二阶隐向量内积的部分 \[\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N{w_ix_i}+\sum_{i=1}^N\sum_{j=i+1}^N{v_i^tv_j\cdot x_ix_j}\] \[\hat{y}_{NFM}(x)=w_0+\sum_{i=1}^{N}w_ix_i+f(x)\] ​ $f(x)$的构造工作可以交由某个深度学习网络来完成，并且通过BP来学习。 ​ NFM的这个神经网络架构特点：在Embedding层和多层神经网络之间加入特征交叉池化层。 ​ 若把NFM的一阶部分看作一个线性模型，则NFM架构也可以视为Wide&amp;amp;Deep模型的进化。NFM模型对Wide&amp;amp;Deep模型的Deep部分加入了特征交叉池化层，加强了特征交叉。 3.7.4 基于FM的深度学习模型的优点和局限性 ​ FNN、DeepFM、NFM都是在经典的多层神经网络的基础上加入有针对性的特征交叉操作，使模型具有更强的非线性表达能力。 ​ 特征工程的思路已经穷尽了可能的尝试，提升空间很小。 3.8 注意力机制在推荐模型中的应用 3.8.1 AFM——引入注意力机制的FM ​ 注意力机制，基于假设——“不同的交叉特征”对于结果的影响程度不同。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现的。注意力网络的作用是为每一个交叉特征提供权重（注意力得分） ​ 为了防止交叉特征数据稀疏带来权重参数难以收敛的问题，AFM使用了一个粥两两特征交叉层和池化层之间的注意力网络来生成注意力得分。 3.8.2 DIN——引入注意力机制的深度学习网络 ​ 应用场景：阿里巴巴电商广告推荐。 ​ 计算一个用户u是否点击广告a时，模型的输入特征分为两类： 用户u的特征组。用户特征组里的商品id/商铺id序列，表示用户点击过的商品/商铺集合。 候选广告a的特征组。广告特征里的商品id和商铺id代表：广告对应的商品id和商铺id。 利用候选商品和历史性为商品之间的相关性计算出一个权重，该权重就代表了“注意力”的强弱。注意力权重+深度学习网络=DIN模型，注意力形式化表达如下： \(V_u = f(V_a)=\sum_{i=1}^{N}{w_i \cdot V_i}=\sum_{i=1}^{N}{g(V_i, V_a)} \cdot V_i\) $V_i$是用户的Embedding向量。$V_a$是候选广告商品的Embedding向量。$V_i$是用户u的第i次行为的Embedding向量。 用户的行为就是浏览商品和店铺，因此行为的Embedding向量就是那次浏览商品或者店铺的Embedding向量。 $g(V_i,V_a)$使用一个激活单元activation unit来生成注意力得分，输入层是两个Embedding向量，经过元素减操作后，与原Embedding向量一同连接后形成全链接层的输入，最后通过单神经元输出层生成注意力得分。 3.8.3 注意力机制对推荐系统的启发 ​ 注意力机制在数学形式上，只是将过去的平均操作或加和操作 换成了 加权求和或加权平均操作。 3.9 DIEN——序列模型与推荐系统的结合</summary></entry><entry><title type="html">template</title><link href="https://duanyc.top//2021/01/23/template.html" rel="alternate" type="text/html" title="template" /><published>2021-01-23T00:00:00+08:00</published><updated>2021-01-23T00:00:00+08:00</updated><id>https://duanyc.top//2021/01/23/template</id><content type="html" xml:base="https://duanyc.top//2021/01/23/template.html">&lt;p&gt;This is an article template.&lt;/p&gt;</content><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><category term="template" /><summary type="html">This is an article template.</summary></entry><entry><title type="html">Welcome</title><link href="https://duanyc.top//2018/07/01/welcome.html" rel="alternate" type="text/html" title="Welcome" /><published>2018-07-01T00:00:00+08:00</published><updated>2018-07-01T00:00:00+08:00</updated><id>https://duanyc.top//2018/07/01/welcome</id><content type="html" xml:base="https://duanyc.top//2018/07/01/welcome.html">&lt;p&gt;If you see this page, that means you have setup your site. enjoy! :ghost: :ghost: :ghost:&lt;/p&gt;

&lt;p&gt;You may want to &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/docs/en/configuration&quot;&gt;config the site&lt;/a&gt; or &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/docs/en/writing-posts&quot;&gt;writing a post&lt;/a&gt; next. Please feel free to &lt;a href=&quot;https://github.com/kitian616/jekyll-TeXt-theme/issues&quot;&gt;create an issue&lt;/a&gt; or &lt;a href=&quot;mailto:kitian616@outlook.com&quot;&gt;send me email&lt;/a&gt; if you have any questions.&lt;/p&gt;

&lt;!--more--&gt;

&lt;hr /&gt;

&lt;p&gt;If you like TeXt, don’t forget to give me a star. :star2:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kitian616/jekyll-TeXt-theme/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/kitian616/jekyll-TeXt-theme.svg?label=Stars&amp;amp;style=social&quot; alt=&quot;Star This Project&quot; /&gt;&lt;/a&gt;&lt;/p&gt;</content><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><category term="TeXt" /><summary type="html">If you see this page, that means you have setup your site. enjoy! :ghost: :ghost: :ghost: You may want to config the site or writing a post next. Please feel free to create an issue or send me email if you have any questions.</summary></entry><entry><title type="html">Post with Header Image</title><link href="https://duanyc.top//2018/06/01/header-image.html" rel="alternate" type="text/html" title="Post with Header Image" /><published>2018-06-01T00:00:00+08:00</published><updated>2018-06-01T00:00:00+08:00</updated><id>https://duanyc.top//2018/06/01/header-image</id><content type="html" xml:base="https://duanyc.top//2018/06/01/header-image.html">&lt;p&gt;A Post with Header Image, See &lt;a href=&quot;https://tianqi.name/jekyll-TeXt-theme/samples.html#page-layout&quot;&gt;Page layout&lt;/a&gt; for more examples.&lt;/p&gt;

&lt;!--more--&gt;</content><author><name>DuanYuchen</name><email>duanyuchen55@gmail.com</email></author><category term="TeXt" /><summary type="html">A Post with Header Image, See Page layout for more examples.</summary></entry></feed>