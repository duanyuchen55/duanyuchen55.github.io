---
title: 《深度学习推荐系统》笔记
tags: 
    - DL
    - RS
mathjax: true
mathjax_autoNumber: true
comment: true
key: 《深度学习推荐系统》笔记
---

[toc]

# 第一章、互联网的增长引擎——推荐系统



# 第二章、前深度学习时代——推荐系统的进化之路



# 第三章、浪潮之巅——深度学习在推荐系统中的应用

## 3.1 深度学习推荐模型的演化关系图

## 3.2 AutoRec——单隐层神经网络推荐模型

## 3.3 Deep Crossing模型——经典的深度学习架构

## 3.4 NeuralCF模型——CF与深度学习的结合

## 3.5 PNN模型——加强特征交叉能力

## 3.6 Wide&Deep模型——记忆能力和泛化能力的综合



## 3.7 FM与深度学习模型的结合

### 3.7.1 FNN——用FM的隐向量完成Embedding层初始化

​		神经网络的参数初始化通常采用不包含任何先验信息的随机初始化，而Embedding层的输入极端稀疏化，导致Embedding层收敛缓慢，且Embedding层参数量占绝大部分，进而导致模型收敛受限于Embedding层。

​		FNN模型解决上述问题的思路：使用FM模型训练好的各特征隐向量初始化Embedding层的参数（引入先验信息）**书p79：图3-8及下面第一段话**

​		FNN模型也为Embedding预训练提供了借鉴思路。

​		

### 3.7.2 DeepFM——用FM代替Wide部分

​		FNN把FM的训练结果作为初始化权重，并未对神经网络的结构进行更改。

​		DeepFM对Wide&Deep的改进在于：用FM替换了Wide部分，加强了浅层网络部分特征组合的能力。**书p80：图3-9及下面第一段话**

​		针对Wide&Deep的改进动机，DeepFM和Deep&Cross完全一样，只不过进行特征组合的方法不一样，前者使用FM，后者使用多层Cross网络。



### 3.7.3 NFM——FM的神经网络化尝试

​		在数学形式上，NFM模型的主要思路是用一个表达能力更强的函数代替FM中的二阶隐向量内积的部分
$$
\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N{w_ix_i}+\sum_{i=1}^N\sum_{j=i+1}^N{v_i^tv_j\cdot x_ix_j}
$$

$$
\hat{y}_{NFM}(x)=w_0+\sum_{i=1}^{N}w_ix_i+f(x)
$$


​		$f(x)$的构造工作可以交由某个深度学习网络来完成，并且通过BP来学习。

​		NFM的这个神经网络架构特点：在Embedding层和多层神经网络之间加入特征交叉池化层。

​		若把NFM的一阶部分看作一个线性模型，则NFM架构也可以视为Wide&Deep模型的进化。NFM模型对Wide&Deep模型的Deep部分加入了特征交叉池化层，加强了特征交叉。



### 3.7.4 基于FM的深度学习模型的优点和局限性

​		FNN、DeepFM、NFM都是在经典的多层神经网络的基础上加入有针对性的特征交叉操作，使模型具有更强的非线性表达能力。

​		特征工程的思路已经穷尽了可能的尝试，提升空间很小。



## 3.8 注意力机制在推荐模型中的应用

### 3.8.1 AFM——引入注意力机制的FM

​		注意力机制，基于假设——“不同的交叉特征”对于结果的影响程度不同。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现的。注意力网络的作用是为每一个交叉特征提供权重（注意力得分）

​		为了防止交叉特征数据稀疏带来权重参数难以收敛的问题，AFM使用了一个粥两两特征交叉层和池化层之间的注意力网络来生成注意力得分。



### 3.8.2 DIN——引入注意力机制的深度学习网络

​		应用场景：阿里巴巴电商广告推荐。

​		计算一个用户u是否点击广告a时，模型的输入特征分为两类：

1. 用户u的特征组。用户特征组里的商品id/商铺id序列，表示用户点击过的商品/商铺集合。
2. 候选广告a的特征组。广告特征里的商品id和商铺id代表：广告对应的商品id和商铺id。

利用候选商品和历史性为商品之间的相关性计算出一个权重，该权重就代表了“注意力”的强弱。注意力权重+深度学习网络=DIN模型，注意力形式化表达如下：
$$
V_u = f(V_a)=\sum_{i=1}^{N}{w_i \cdot V_i}=\sum_{i=1}^{N}{g(V_i, V_a)} \cdot V_i
$$
$V_i$是用户的Embedding向量。$V_a$是候选广告商品的Embedding向量。$V_i$是用户u的第i次行为的Embedding向量。

用户的行为就是浏览商品和店铺，因此行为的Embedding向量就是那次浏览商品或者店铺的Embedding向量。

$g(V_i,V_a)$使用一个激活单元activation unit来生成注意力得分，输入层是两个Embedding向量，经过元素减操作后，与原Embedding向量一同连接后形成全链接层的输入，最后通过单神经元输出层生成注意力得分。



### 3.8.3 注意力机制对推荐系统的启发

​		注意力机制在数学形式上，只是将过去的平均操作或加和操作 换成了 加权求和或加权平均操作。



## 3.9 DIEN——序列模型与推荐系统的结合

### 3.9.1 DIEN的“进化”动机

​	序列信息的重要性在于：

1. 它加强了最近行为对下次行为预测的影响。
2. 序列模型可以学习到购买趋势的信息。

### 3.9.2 DIEN模型的架构

​	”兴趣进化网络“被认为是一种用户兴趣的Embedding方法，最终删除是$h'(T)$这个用户兴趣向量，DIEN模型创新点在于如何构建”兴趣进化网络“。

​	兴趣进化网络分为三层，从下至上依次是：

1. **行为序列层：**主要作用是把原始的id类行为序列转换成Embedding行为序列。
2. **兴趣抽取层：**主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣。
3. **兴趣进化层：**主要作用是在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。

### 3.9.3 兴趣抽取层的结构

​	兴趣抽取层的基本结构是GRU网络。相比于传统的序列模型RNN和LSTM，GRU解决了RNN的梯度消失问题。GRU参数更少，收敛速度更快，所以被DIEN序列模型的选择。

​	经过由GRU组成的兴趣抽取层后，用户的行为向量$b(t)$被进一步抽象化，形成了兴趣状态向量$h(t)$。
$$
u_t = \sigma(W^u i_t + U^uh_{t-1} + b^u) \\
r_t = \sigma(W^r i_t + U^rh_{t-1} + b^r) \\
\widetilde h_t = tanh(W^hi_t + r_t \circ U^h h_{t-1} + b^h) \\
h_t = (1-u_t) \circ h_{t-1} + u_t \circ \widetilde h_t
$$


### 3.9.4 兴趣进化层的结构

​	DIEN兴趣进化层相比兴趣抽取层最大的特点是加入了注意力机制（与DIN一样）。兴趣进化层注意力得分的生成过程与DIN完全一致，都是当前状态向量与目标广告向量进行相互作用的结果。**即，DIEN在模拟兴趣进化的过程中，需要考虑与目标广告的相关性。**

​	在兴趣抽取层智商加上了兴趣净化层就是为了**更有针对性地模拟与目标广告相关的兴趣进化路径**。

​	兴趣进化层完成注意力机制的引入是通过AUGRU（GRU with Attentional Update gate，基于注意力更新门的GRU）结构，AUGRU在原GRU的更新门的结构上加入了注意力得分$a_t$。
$$
\widetilde u_t' = a_t \cdot u_t'\\
 h_t'= (1- \widetilde u_t') \circ h_{t-1}' + \widetilde u_t' \circ  \widetilde h_t'
$$

### 3.9.5 序列模型对推荐系统的启发

序列模型具备强大的时间序列表达能力，非常适合预估用户经过一系列行为后的下一动作。

工程上，序列模型有比较高的 训练复杂度，以及在线上推断过程中的串行推断，使其在模型服务过程中延迟较大 ，增大了上线难度，需要在工程上着重优化。



## 3,10 强化学习与推荐系统的结合

强化学习（Reinforcement Learning）起源于机器人领域，针对智能体（Agent）在不断变化的环境（Environment）中，决策和学习的过程进行建模。在智能体学习过程中，会完成收集外部反馈（Reward），改变自身状态（State），再根据自身状态对下一步的行动（Action）进行决策，在行动之后持续收集反馈的循环。简称“行动-反馈-状态更新”的循环。

### 3.10.1 深度强化学习 推荐系统框架

- 智能体：推荐系统本身，包括基于DL的推荐模型，探索策略，数据存储
- 环境：由新闻网站或者APP、用户组成的整个推荐系统外部环境
- 行动：对一个新闻推荐系统来说，行动指的就是推荐系统进行新闻排序后推送给用户的动作
- 反馈：用户收到推荐结果后，进行正向或负向的反馈。
- 状态：对环境及自身当前所处具体情况的刻画。（当前已收到的所有的动作和反馈以及用户和新闻的所有相关信息的特征向量表示）

迭代过程如下：

1. 初始化推荐系统（智能体）
2. 推荐系统基于当前手机的数据（状态）进行新闻排序（行动），并且推送到网站或者APP（环境）
3. 用户收到推荐列表，点击/忽略（反馈）某推荐结果。
4. 推荐系统收到反馈，更新当前状态或通过模型训练更新模型。
5. 重复2

### 3.10.2 深度强化学习 推荐模型

DRN中，智能体是Deep Q-Network（深度Q网络，DQN），Q是Quality，指的是通过对行动进行质量评估，得到行动的效用得分，以此进行行动决策。

DQN在特征工程中，套用强化学习状态向量和行动向量的概念，将用户特征user features和环境特征context features归为状态向量，将用户-新闻特征归为行动特征。

### 3.10.3 DRN的学习过程

DRN可以在线更新，和其他“静态”深度学习模型相比，有实时性优势。

按照时间顺序，DRN学习过程的重要步骤：

1. 离线部分，根据历史数据训练好DQN模型，作为智能体的初始化模型
2. 在$t_1 \rightarrow t_2$阶段，利用初始化模型进行一段时间的推送服务，积累反馈数据
3. 在$t_2$时间点，利用$t_1 \rightarrow t_2$阶段积累的用户点击数据，进行模型微更新（minor update）
4. 在$t_4$时间点，利用$t_1 \rightarrow t_4$阶段的用户点击数据及用户活跃度数据进行模型的主更新（major update）
5. 重复2~4步骤

第四步可以理解为使用历史数据重新训练。第三步的模型微调联系到了DRN的在线训练算法——竞争梯度下降算法

### 3.10.4 DRN的在线学习方法——竞争梯度下降算法

步骤：

1. 对于已经训练好的网络Q，对其模型参数W添加一个较小的随机扰动$\Delta W$得到新的模型参数$\widetilde W$，$\widetilde W$ 对应的网络成为探索网络$\widetilde Q$。
2. 对于当前网络Q和探索网络$\widetilde Q$，分别生成推荐列表L和$\widetilde L$，将这两个推荐列表合成一个推荐列表后推送给用户。
3. 实时收集用户的反馈，如果探索网络生成的内容由于当前网络，这用探索网络替代当前网络，进入下一轮迭代，否则保留当前网络。

第一步，由当前网络生成探索网络，产生随机扰动的公式如下：
$$
\Delta W = \alpha \cdot rand(-1,1) \cdot W
$$
$\alpha$是探索因子，决定探索力度的大小。$rand(-1,1)$是[-1,1]之间的一个随机数。

### 3.10.5 强化学习对推荐系统的启发

变静态为动态，把模型学习的实时性变得很重要。

Question：重量级&完美&延迟大 v.s. 轻巧&简单&实时训练

# 第四章、Embedding技术在推荐系统中的应用

​	Embedding中文直译：嵌入，向量化，向量映射。

​	Embedding的主要作用：将稀疏向量转换成稠密向量，便于上层深度神经网络处理。



## 4.1 什么是Embedding

​	形式上，**Embeding是用一个低位稠密的向量“表示”一个对象object**，即Embedding向量能够表达相应对象的某些特征，同时向量之间的距离反映了对象之间的相似性。

### 4.1.1 词向量的例子

​	Embedding向量之间的运算甚至能够包含词之间的语义关系信息。在有大量语料输入的前提下，Embedding技术甚至可以挖掘出一些通用知识。

### 4.1.2 Embedding技术在其他领域的扩展

​	与词向量使用大量文本语料进行训练不同，不同领域的训练样本肯定不同，如视频推荐使用用户的观看序列进行电影的Embedding化，而电商平台使用用户的购买历史作为训练样本。

### 4.1.3 Embedding技术对于深度学习推荐系统的重要性

1. 推荐场景中大量使用one-hot对类别，id型特征进行编码，导致样本特征向量稀疏，不利于深度学习处理，所以需要由Embedding层负责将高维稀疏特征向量转换成低维稠密特征向量。
2. Embedding本身就是重要的特征向量，表达能力更强。
3. Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。适用于对海量备选物品进行快速初筛。



## 4.2 Word2vec——经典的Embedding方法

### 4.2.1 什么是Word2vec

​	word to vector，是一个生成对“词”的向量表达的模型。

​	假设由一组句子组成的语料库，其中的一个长度为$T$的句子为$w_1, w_2,\cdots,w_T$，假定每个词都跟其最相邻的词的关系最密切，每个词都是由相邻的词决定的（CBOW），或者每个词都决定了相邻的词（skip-gram）。
​	CBOW模型的输入是$w_T$周围的词，预测的输出是$w_t$，而Skip-gram则相反。

### 4.2.2 Word2vec模型的训练过程

- 训练样本：选取一个长度为2c+1的滑动窗口，从语料库中抽取一个句子，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就组成了一个训练样本。
- 优化目标：每个词$w_t$都决定了相邻的词$w_{t+j}$，基于极大似然估计的方法，希望所有样本的条件概率 $p(w_{t+j}\|w_t)$ 之积最大，这里使用对数概率，Word2vec的目标函数如下：

$$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c\leq j\leq c,j \neq 0} log p(w_{t+j}|w_t)
$$



- $p(w_{t+j}\|w_t)$如何定义：多分类问题，最直接的是使用Softmax函数。$p(w_{t+j}\|w_t)$定义如下：

$$
p(W_O|W_I)=\frac{exp(V'_{W_O}{\mathsf{T}} V_{W_I})}{\sum_{w=1}^{W}exp(V'_{W}{\mathsf{T}} V_{W_I})}
$$

$w_O$代表$w_{t+j}$，被称为输出词，$w_I$代表$w_t$，被称为输入词。

输入向量表达是输入层到隐层的权重矩阵$W_{V \times N}$，而输出向量表达是隐层到输出层的权重矩阵$W'_{N\times V}$。

​	在获得输入向量矩阵$W_{V \times N}$后，每一行对应的权重向量就是“词向量”。



### 4.2.3 Word2vec的“负采样”训练方法

原始的Word2vec计算量过大，往往采用负采样的方法进行训练，只需要对采样出的几个负样本计算预测误差。Word2vec模型的优化目标从一个读哟分类问题退化成了一个近似二分类问题，式子如下：
$$
E = -log \sigma (v'_{w_o}{\mathrm{T}} h - \sum_{W_j \in W_{neg}}log \sigma(-v'_{w_j}{\mathrm{T}} h))
$$
$v'_{w_o}$是输出词向量（正样本），h是隐层向量，$W_{neg}$是负样本集合，$v'_{w_j}$是负样本词向量。福特样本集合的大小很有限，通常小于10。

### 4.2.4 Word2vec对Embedding技术的奠基性意义





## 4.3 Item2vec——Word2vec在推荐系统领域的推广

### 4.3.1 Item2vec的基本原理

由于Word2vec的流行，越来越多的Embedding方法可以被直接用于**物品Embedding向量**的生成。

**用户Embedding向量**这可以通过行为历史中的物品Embedding平均或者聚类得到。



利用用户向量和物品向量的相似性，可以直接在推荐系统的召回层快速得到候选集合，或在排序层直接用于最终推荐列表的排序。

1. Word2vec利用词序列生成词Embedding。

   Item2vec利用的物品序列是由特定用户浏览、购买等行为产生的历史行为记录序列。

2. Word2vec中一个长度为T的句子为$w_1,w_2,...,w_T$，则其优化目标如公式7所示
   假设Item2vec中一个长度为K的用户历史记录为$w_1,w_2,...,w_K$，类比前者，优化目标如下所示：
   $$
   \frac{1}{K} \sum_{t=1}^{K} \sum_{j \neq i} log p(w_{j}|w_i)
   $$
   通过观察两个公式可以看到不同在于，Item2vec丢弃了时间窗口的概念，认为序列中任意两个物品都相关，所以在公式中，是两两物品的对数概率的和，而不仅仅是时间窗口内物品的对数概率之和。

3. 优化目标定义好后，其余的训练过程和最终物品Embedding的产生过程，Item2vec与Word2vec完全一致。最终物品向量的查找表就是词向量的查找表。

### 4.3.2 “广义”的Item2vec

广义上讲，任何可以生成物品向量的方法都可以称作Item2vec，eg双塔模型。

广告侧的模型是对物品进行Embedding，称作“物品塔”，本质是接受物品相关的特征向量。最后输出一个多维的稠密向量

### 4.3.3 Item2vec方法的特点和局限性

优点：作为Word2vec的推广，理论上可以使用任何序列型数据生成物品的 Embedding向量，大大拓展了Word2vec的使用场景

局限性：只能利用序列型数据，无法处理网络化数据——>Graph Embedding技术出现



## 4.4 Graph Embedding——引入更多结构信息和图嵌入技术

Graph Embedding是一种对图结构中的节点进行Embedding编码的方法。**最终生成的节点Embedding向量包含图的结构信息及附近节点的局部相似性信息。**

### 4.4.1 DeepWalk——基础的Graph Embedding方法

主要思想：在由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入Word2vec进行训练，得到物品的Embedding。**是链接序列Embedding和Graph Embedding的过渡方法。**

DeepWalk算法流程：

1. 原始用户行为序列
2. 基于用户行为序列，构建物品关系图。如果后续产生多条相同的有向边，则有向边的权重加强。将所有用户行为序列都转换为物品关系图中的边之后，全局的物品关系图就建立起来了
3. 采用随机游走的方式，随机选择起始点，重新产生物品序列。
4. 将这些物品序列输入Word2vec模型中，生成最终的物品Embedding向量。

随机游走的跳转概率（到达节点$v_i$后，下一步遍历$v_i$的邻接点$v_j$的概率）:

​	若物品关系图是有向有权图，从节点$v_i$跳转到$v_j$的概率为：
$$
P(v_j|v_i)= 
\left\{  
             \begin{array}{cc}  
              \frac{M_{ij}}{\sum_{j\in N_+(v_i)}M_{ij}}, & v_j \in N_+(v_i) \\
              0, & e_{ij} \notin \varepsilon
             \end{array}  
\right.
$$
$\varepsilon$是物品关系图中所有边的集合，$ N_+(v_i)$是节点$v_i$所有的出边集合，$M_{ij}$是节点$v_i$到$v_j$边的权重。**==即，DeepWalk的跳转概率是跳转边的权重站所有相关出边的权重之和的比例==**

若物品关系图是无向无权图，则$M_{ij}$为1，且$ N_+(v_i)$是节点$v_i$的所有边的集合。

### 4.4.2 Node2vec——同质性和结构性的权衡

在DeepWalk的基础上，调整随机游走权重，使得Graph Embedding的结果更加体现网络的同质性和结构性。

- 网络的同质性：距离相近的节点的Embedding尽量相似
- 结构性：结构上相似的节点的Embedding应尽量相似

为了让Graph Embedding的结果可以表达网络的**结构性**，在随机游走过程中，需要让游走更接近于BFS，更多地在当前节点的领域中遍历，相当于对当前节点周边的网络结构进行一次“微观扫描”。
当前节点是 局部中心点、边缘节点或者连接性节点，其序列包含的节点数量和顺序必然是不同的，从而让最终的Embedding抓取到更多结构性信息。

为了让Graph Embedding的结果可以表达网络的**同质性**，需要让游走更接近于DFS，因为DFS可以游走到远方的节点，DFS的游走在一个大的集团内，使得一个集团或者社区内部的节点Embedding更为相似。

---

在Node2vec中，是通过节点间的跳转概率来控制DFS和DFS的倾向性。

从节点v跳转到下一个节点x的概率$\pi_{vx} = \alpha (t,x) \cdot \omega _{vx}$，$\omega _{vx}$是边vx的权重，$\alpha (t,x)$定义如下：
$$
\alpha (t,x) = 
\left\{
	\begin{array}{}
		\frac{1}{p}, & d_{tx}=0 \\
		1, & d_{tx}=1 \\
		\frac{1}{q}, & d_{tx}=2 \\
	\end{array}
\right.
$$
其中$d_{tx}$是节点t到节点x的距离，参数p和q共同控制随机游走的倾向性。

- 参数p为返回参数return parameter，p越小，随即游走回节点t的可能性越大，Node2vec网络就更注重表达网络的结构性；

- 参数q为进出参数in-out parameter，q越小，随即游走到远方节点的可能性越大，Node2vec更注重表达网络的同质性。

注重同质性，距离相近的节点颜色更接近；注重结构性，结构特点相近的节点颜色更接近。

同质性相同的可能是同品类、同时性或者被一同购买的商品；结构性相同的商品可能是各品类的爆款、各品类的最佳凑单款等拥有类似趋势或结构属性的商品。

### 4.4.3 EGES——阿里巴巴的综合性Graph Embedding方法

在淘宝应用的Embedding方法**EGES(Enhanced Graph Embedding with Side Information)**，基本思想是在DeepWalk生成的Graph Embedding基础上引入补充信息。

仅仅使用用户行为生成的物品相关图，在遇到新加入的物品，或者没有很多互动信息的长尾物品时，推荐系统会产生**冷启动问题**。为了使冷启动商品获得合理的初始Embedding，通过引入更多的补充信息(side information)来丰富Embedding信息的来源，从而使没有历史行为记录的商品获得合理的初始Embedding。

生成Graph Embedding的第一步是生成物品关系图，除了通过用户行为序列，还可以利用相同属性，相同类别等信息也可以建立物品之间的边，得到**基于内容的知识图谱**。基于知识图谱生成的物品向量可以被称为**补充信息Embedding向量**。

如何融合一个物品的多个Embedding向量？
最简单的方法，是在深度神经网络中加入平均池化层，将不同Embedding平均起来。为了防止简单的平均池化导致有效信息丢失，可以对每个Embedding加权重（类似于DIN的注意力机制），对每类特征对应的Embedding向量，赋予不同的权重。详见p117 图4-11。

---

实际模型中，使用的是$e^{a_j}$而不是$a_j$，一是避免权重为0，二是$e^{a_j}$在梯度下降过程中有良好的数学性质。

EGES没有复杂的理论创新，但是给出了一个融合多种Embedding的方法，降低了某类信息缺失造成的冷启动问题。

### 4.5 Embedding与深度学习推荐系统的结合

Embedding的主要应用方向：

1. 在深度学习网络中作为Embedding层，完成从高维稀疏特征向量到低位稠密特征向量的转换，
2. 作为预训练的Embedding特征向量，与其他特征向量连接后，一同输入深度学习网络进行训练
3. 通过计算用户和物品的Embedding相似度，Embedding可以直接作为推荐系统的召回层或者召回策略之一。

### 4.5.1 深度学习网络中的Embedding层

若使用深度学习模型处理高维稀疏特征向量，都会在输入层到全连接层之间加入Embedding层，完成向低位稠密特征向量的转换。

DeepCrossing、FNN、Wide&Deep的Embedding层接受的都是类别型特征的one-hot向量，转换为低维的Embedding向量。Embedding层是一个高维向量向低维向量的映射。

用矩阵的形式表达Embedding层，本质上是求解一个$m \times n$(输入高维稀疏向量 x 输出低维稠密向量)的权重矩阵的过程。

理论上，将Embedding层和整个深度学习网络整合后一同训练是最优选择，但是由于Embedding层参数量巨大，这样做会拖慢整个神经网络的收敛速度。**所以工程上要求 快速更新的深度学习推荐系统放弃了Embedding层的端到端训练，用与训练Embedding层的方式来替代。**

### 4.5.2 Embedding的预训练方法

Embedding的训练往往独立于深度学习网络进行，得到稠密表达之后，再与其他特征一起训练

典型的采用Embedding预训练方法的模型是FNN，将FM模型训练得到各特征隐向量初始化Embedding层的初始化权重。如果想要进一步加快收敛速度，还可以采用”固定Embedding层权重，仅更新上层神经网络权重“的方法。

**使用不同的训练频率来更新Embedding模型和神经网络模型，是训练开销和模型效果之间权衡后的最佳方案。**

### 4.5.3 Embedding作为推荐系统召回层的方法

利用Embedding向量的相似性，将Embedding作为推荐系统的召回层。

模型输入层特征全部都是用户相关特征，模型输出层是softmax层，模型本质是一个多分类模型，预测目标是用户观看了哪个视频，所以softmax层的输入是经过三层ReLU全连接层生成的用户Embedding，输出向量是用户观看每一个视频的概率分布。由于输出向量的每一维对应了一个视频，该维对应的softmax层列向量就是该物品的Embedding。

在模型部署过程中，只需要将用户和物品的Embedding存储到线上内存数据库，通过内积运算再排序的方法，就可以得到物品的排序，再通过取序列中TopN的物品即可得到召回的候选集合。

但是这种遍历内积运算的O(n)级别的方法依然还是很慢，工程上需要一种针对相似Embedding的快速索引方法，更快地召回候选集合。  

## 4.6 局部敏感哈希——让Embedding插上翅膀的快速搜索方法

推荐系统召回层的主要功能是快速地将推荐物品的候选集从百万量级的规模减小到几千几百量级的规模，避免将全部候选物品直接输入深度学习模型造成的计算资源浪费和预测延迟的问题。

### 4.6.1 “快速”Embedding最近邻搜索

召回与用户向量最相似的物品Embedding向量的过程其实是在一个向量空间内搜索最近邻的过程。

通过建立kd树索引结构进行最近邻搜索是常用的快速最近邻搜索方法，但kd树结构复杂，搜索过程复杂且时间复杂度不是最优$\longrightarrow$ $\longrightarrow$局部敏感哈希。

### 4.6.2 局部敏感哈希的基本原理

基本思想：让相邻的点落入同一个“桶”，这样可以再最近邻搜索时候，仅在一个桶内、或临近的几个桶内进行搜索。

以基于欧氏距离的最近邻搜索为例，解释构建局部敏感哈希“桶”的过程。

在欧式空间中，将高维空间的点映射到低维空间，原本近的点，依然近，远的点，未必远。

利用低维空间可以保留高维空间距离相近关系的性质，就可以构造局部敏感哈希“桶”。

内积操作可以将v映射到一维空间，成为一个数值$h(v)=v \cdot x$，v是高维空间中的k维Embedding向量，x是随机生成的k维映射向量。

可以使用如下所示哈希函数$h(v)$进行分桶：
$$
h^{x,b}(v) = \lfloor \frac{v \cdot x}{w} \rfloor
$$
w是分桶宽度，b是0到w间的一个均匀分布随机变量，避免分桶边界固化。

注：

映射操作损失了部分距离信息，所以仅仅采用一个哈希函数进行分桶，必然存在相近点误判的情况。解决办法就是使用m个哈希函数同事进行分桶，同时掉进m个哈希函数的同一个桶的两点，是相似点的概率大大增加。通过分桶找到相邻点的候选集合后，就可以在有限的候选集合中通过遍历找到目标点真正的K近邻。

### 4.6.3 局部敏感哈希多桶策略

采用多个哈希函数进行分桶，有一个问题，是通过and还是or来生成最终的候选集。and准确率提高，但候选集的规模减小减小整体的开销。or候选集召回率提高，规模变大开销增大。

使用几个哈希函数，使用or还是and需要在准确率和召回率之间作出权衡。

# 第五章、多角度审视推荐系统

# 第六章、深度学习推荐系统的工程实现



# 第七章、推荐系统的评估



# 第八章、深度学习推荐系统的前沿实践



# 第九章、构建属于你自己的推荐系统知识框架









