---
title: 《深度学习推荐系统》笔记
tags: 
    - DL
    - RS
mathjax: true
mathjax_autoNumber: true
comment: true
key: 《深度学习推荐系统》笔记
---

@[toc]



# 第一章、互联网的增长引擎——推荐系统



# 第二章、前深度学习时代——推荐系统的进化之路



# 第三章、浪潮之巅——深度学习在推荐系统中的应用

## 3.1 深度学习推荐模型的演化关系图

## 3.2 AutoRec——单隐层神经网络推荐模型

## 3.3 Deep Crossing模型——经典的深度学习架构

## 3.4 NeuralCF模型——CF与深度学习的结合

## 3.5 PNN模型——加强特征交叉能力

## 3.6 Wide&Deep模型——记忆能力和泛化能力的综合



## 3.7 FM与深度学习模型的结合

### 3.7.1 FNN——用FM的隐向量完成Embedding层初始化

​		神经网络的参数初始化通常采用不包含任何先验信息的随机初始化，而Embedding层的输入极端稀疏化，导致Embedding层收敛缓慢，且Embedding层参数量占绝大部分，进而导致模型收敛受限于Embedding层。

​		FNN模型解决上述问题的思路：使用FM模型训练好的各特征隐向量初始化Embedding层的参数（引入先验信息）**书p79：图3-8及下面第一段话**

​		FNN模型也为Embedding预训练提供了借鉴思路。

​		

### 3.7.2 DeepFM——用FM代替Wide部分

​		FNN把FM的训练结果作为初始化权重，并未对神经网络的结构进行更改。

​		DeepFM对Wide&Deep的改进在于：用FM替换了Wide部分，加强了浅层网络部分特征组合的能力。**书p80：图3-9及下面第一段话**

​		针对Wide&Deep的改进动机，DeepFM和Deep&Cross完全一样，只不过进行特征组合的方法不一样，前者使用FM，后者使用多层Cross网络。



### 3.7.3 NFM——FM的神经网络化尝试

​		在数学形式上，NFM模型的主要思路是用一个表达能力更强的函数代替FM中的二阶隐向量内积的部分
$$
\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N{w_ix_i}+\sum_{i=1}^N\sum_{j=i+1}^N{v_i^tv_j\cdot x_ix_j}
$$

$$
\hat{y}_{NFM}(x)=w_0+\sum_{i=1}^{N}w_ix_i+f(x)
$$


​		$f(x)$的构造工作可以交由某个深度学习网络来完成，并且通过BP来学习。

​		NFM的这个神经网络架构特点：在Embedding层和多层神经网络之间加入特征交叉池化层。

​		若把NFM的一阶部分看作一个线性模型，则NFM架构也可以视为Wide&Deep模型的进化。NFM模型对Wide&Deep模型的Deep部分加入了特征交叉池化层，加强了特征交叉。



### 3.7.4 基于FM的深度学习模型的优点和局限性

​		FNN、DeepFM、NFM都是在经典的多层神经网络的基础上加入有针对性的特征交叉操作，使模型具有更强的非线性表达能力。

​		特征工程的思路已经穷尽了可能的尝试，提升空间很小。



## 3.8 注意力机制在推荐模型中的应用

### 3.8.1 AFM——引入注意力机制的FM

​		注意力机制，基于假设——“不同的交叉特征”对于结果的影响程度不同。AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络实现的。注意力网络的作用是为每一个交叉特征提供权重（注意力得分）

​		为了防止交叉特征数据稀疏带来权重参数难以收敛的问题，AFM使用了一个粥两两特征交叉层和池化层之间的注意力网络来生成注意力得分。



### 3.8.2 DIN——引入注意力机制的深度学习网络

​		应用场景：阿里巴巴电商广告推荐。

​		计算一个用户u是否点击广告a时，模型的输入特征分为两类：

1. 用户u的特征组。用户特征组里的商品id/商铺id序列，表示用户点击过的商品/商铺集合。
2. 候选广告a的特征组。广告特征里的商品id和商铺id代表：广告对应的商品id和商铺id。

利用候选商品和历史性为商品之间的相关性计算出一个权重，该权重就代表了“注意力”的强弱。注意力权重+深度学习网络=DIN模型，注意力形式化表达如下：
$$
V_u = f(V_a)=\sum_{i=1}^{N}{w_i \cdot V_i}=\sum_{i=1}^{N}{g(V_i, V_a)} \cdot V_i
$$
$V_i$是用户的Embedding向量。$V_a$是候选广告商品的Embedding向量。$V_i$是用户u的第i次行为的Embedding向量。

用户的行为就是浏览商品和店铺，因此行为的Embedding向量就是那次浏览商品或者店铺的Embedding向量。

$g(V_i,V_a)$使用一个激活单元activation unit来生成注意力得分，输入层是两个Embedding向量，经过元素减操作后，与原Embedding向量一同连接后形成全链接层的输入，最后通过单神经元输出层生成注意力得分。



### 3.8.3 注意力机制对推荐系统的启发

​		注意力机制在数学形式上，只是将过去的平均操作或加和操作 换成了 加权求和或加权平均操作。



## 3.9 DIEN——序列模型与推荐系统的结合

### 3.9.1 DIEN的“进化”动机

​	序列信息的重要性在于：

1. 它加强了最近行为对下次行为预测的影响。
2. 序列模型可以学习到购买趋势的信息。

### 3.9.2 DIEN模型的架构

​	”兴趣进化网络“被认为是一种用户兴趣的Embedding方法，最终删除是$h'(T)$这个用户兴趣向量，DIEN模型创新点在于如何构建”兴趣进化网络“。

​	兴趣进化网络分为三层，从下至上依次是：

1. **行为序列层：**主要作用是把原始的id类行为序列转换成Embedding行为序列。
2. **兴趣抽取层：**主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣。
3. **兴趣进化层：**主要作用是在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。

### 3.9.3 兴趣抽取层的结构

​	兴趣抽取层的基本结构是GRU网络。相比于传统的序列模型RNN和LSTM，GRU解决了RNN的梯度消失问题。GRU参数更少，收敛速度更快，所以被DIEN序列模型的选择。

​	经过由GRU组成的兴趣抽取层后，用户的行为向量$b(t)$被进一步抽象化，形成了兴趣状态向量$h(t)$。
$$
u_t = \sigma(W^u i_t + U^uh_{t-1} + b^u) \\
r_t = \sigma(W^r i_t + U^rh_{t-1} + b^r) \\
\widetilde h_t = tanh(W^hi_t + r_t \circ U^h h_{t-1} + b^h) \\
h_t = (1-u_t) \circ h_{t-1} + u_t \circ \widetilde h_t
$$


### 3.9.4 兴趣进化层的结构

​	DIEN兴趣进化层相比兴趣抽取层最大的特点是加入了注意力机制（与DIN一样）。兴趣进化层注意力得分的生成过程与DIN完全一致，都是当前状态向量与目标广告向量进行相互作用的结果。**即，DIEN在模拟兴趣进化的过程中，需要考虑与目标广告的相关性。**

​	在兴趣抽取层智商加上了兴趣净化层就是为了**更有针对性地模拟与目标广告相关的兴趣进化路径**。

​	兴趣进化层完成注意力机制的引入是通过AUGRU（GRU with Attentional Update gate，基于注意力更新门的GRU）结构，AUGRU在原GRU的更新门的结构上加入了注意力得分$a_t$。
$$
\widetilde u_t' = a_t \cdot u_t'\\
 h_t'= (1- \widetilde u_t') \circ h_{t-1}' + \widetilde u_t' \circ  \widetilde h_t'
$$

### 3.9.5 序列模型对推荐系统的启发





## 3,10 强化学习与推荐系统的结合

### 3.10.1 深度学习强化学习推荐系统框架

### 3.10.2 深度强化学习推荐模型

### 3.10.3 DRN的学习过程

### 3.10.4 DRN的在线学习方法——竞争梯度下降算法

### 3.10.5 强化学习对推荐系统的启发





# 第四章、Embedding技术在推荐系统中的应用

​	Embedding中文直译：嵌入，向量化，向量映射。

​	Embedding的主要作用：将稀疏向量转换成稠密向量，便于上层深度神经网络处理。



## 4.1 什么是Embedding

​	形式上，**Embeding是用一个低位稠密的向量“表示”一个对象object**，即Embedding向量能够表达相应对象的某些特征，同时向量之间的距离反映了对象之间的相似性。

### 4.1.1 词向量的例子

​	Embedding向量之间的运算甚至能够包含词之间的语义关系信息。在有大量语料输入的前提下，Embedding技术甚至可以挖掘出一些通用知识。

### 4.1.2 Embedding技术在其他领域的扩展

​	与词向量使用大量文本语料进行训练不同，不同领域的训练样本肯定不同，如视频推荐使用用户的观看序列进行电影的Embedding化，而电商平台使用用户的购买历史作为训练样本。

### 4.1.3 Embedding技术对于深度学习推荐系统的重要性

 	1. 推荐场景中大量使用one-hot对类别，id型特征进行编码，导致样本特征向量稀疏，不利于深度学习处理，所以需要由Embedding层负责将高维稀疏特征向量转换成低维稠密特征向量。
 	2. Embedding本身就是重要的特征向量，表达能力更强。
 	3. Embedding对物品、用户相似度的计算是常用的推荐系统召回层技术。适用于对海量备选物品进行快速初筛。



## 4.2 Word2vec——经典的Embedding方法

### 4.2.1 什么是Word2vec

​	word to vector，是一个生成对“词”的向量表达的模型。

​	假设由一组句子组成的语料库，其中的一个长度为$T$的句子为$w_1, w_2,\cdots,w_T$，假定每个词都跟其最相邻的词的关系最密切，每个词都是由相邻的词决定的（CBOW），或者每个词都决定了相邻的词（skip-gram）。
​	CBOW模型的输入是$w_T$周围的词，预测的输出是$w_t$，而Skip-gram则相反。

### 4.2.2 Word2vec模型的训练过程

- 训练样本：选取一个长度为2c+1的滑动窗口，从语料库中抽取一个句子，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就组成了一个训练样本。
- 优化目标：每个词$w_t$都决定了相邻的词$w_{t+j}$，基于极大似然估计的方法，希望所有样本的条件概率$p(w_{t+j}|w_t)$之积最大，这里使用对数概率，Word2vec的目标函数如下：

$$
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c\leq j\leq c,j \neq 0} log p(w_{t+j}|w_t)
$$



- $p(w_{t+j}|w_t)$如何定义：多分类问题，最直接的是使用Softmax函数。$p(w_{t+j}|w_t)$定义如下：

$$
p(W_O|W_I)=\frac{exp(V'_{W_O}{\mathsf{T}} V_{W_I})}{\sum_{w=1}^{W}exp(V'_{W}{\mathsf{T}} V_{W_I})}
$$

$w_O$代表$w_{t+j}$，被称为输出词，$w_I$代表$w_t$，被称为输入词。

输入向量表达是输入层到隐层的权重矩阵$W_{V \times N}$，而输出向量表达是隐层到输出层的权重矩阵$W'_{N\times V}$。

​	在获得输入向量矩阵$W_{V \times N}$后，每一行对应的权重向量就是“词向量”。



### 4.2.3 Word2vec的“负采样”训练方法

### 4.2.4 Word2vec对Embedding技术的奠基性意义



## 4.3 Item2vec——Word2vec在推荐系统领域的推广



## 4.4 Graph Embedding——引入更多结构信息和图嵌入技术



## 4.5 Embedding与深度学习推荐系统的结合



## 4.6 局部敏感哈希——让Embedding插上翅膀的快速搜索方法





# 第五章、多角度审视推荐系统

# 第六章、深度学习推荐系统的工程实现



# 第七章、推荐系统的评估



# 第八章、深度学习推荐系统的前沿实践



# 第九章、构建属于你自己的推荐系统知识框架









